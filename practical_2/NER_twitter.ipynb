{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2c436508",
      "metadata": {
        "id": "2c436508"
      },
      "source": [
        "### Recognizing named entities off tweets using LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-S1keQ7Cc_9c",
      "metadata": {
        "id": "-S1keQ7Cc_9c"
      },
      "source": [
        "The Idea of Named Entity Recognition (NER) is to extract \"named entities\" from a text corpus. Examples of these entities may be person names, location, calendar months, course ID specific to a department etc. <br>\n",
        "So if you were to extract named entities - *person (PER)*, *course ID (C_ID)* of the sentence below: <br>\n",
        "**L645 is taught by Francis Tyers** <br>\n",
        "The NER model that you're going to be building would output a sequence of tags associated with the sentence as shown below: <br>\n",
        "**B-C_ID    O    O    O    B-PER    I-PER**\n",
        "\n",
        "THE B, I and O that you see above represent a prefix scheme known as *BIO markup* The B represents Beginning, I represents Inside and O represents Outside/Out-of. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d_t23qmb0SEe",
      "metadata": {
        "id": "d_t23qmb0SEe"
      },
      "source": [
        "In this notebook, you will use a recurrent neural network (RNN) model to carry out the aforementioned NER task. <br>\n",
        "The notebook has been broken down into 15 different sections that you will run sequentially to complete this exercise. <br> \n",
        "There are few lines of code that you're expected to complete in those different sections to keep you engaged. You'll find them labeled as either **COMPLETE** or **YOUR CODE HERE** in their respective places.\n",
        "\n",
        "Specifically, you're going to want to complete the following -\n",
        "1. Replace '@' with \\<USR\\> token in section 1\n",
        "2. Create mappings from tokens to indices and vice versa in section 2\n",
        "3. Create embedding variable in section 6\n",
        "4. Create softmax and argmax in section 7\n",
        "5. Create adam optimizer in section 8\n",
        "6. Adjust hyperparameters in section 13\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V5C_6PyC1v85",
      "metadata": {
        "id": "V5C_6PyC1v85"
      },
      "source": [
        "We recommend you use [Google Collab](https://colab.research.google.com/) for this exercise. <br>\n",
        "To get started, you're going to want to run the cell below to download the data corpus you'd be using in the exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "01pnSiCG2LFF",
      "metadata": {
        "id": "01pnSiCG2LFF",
        "outputId": "20c9b9ab-0086-439b-8514-b9e6429ca7a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-29 21:51:06--  https://raw.githubusercontent.com/heisenberg967/ner_twitter/main/data/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 849548 (830K) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "train.txt           100%[===================>] 829.64K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2022-09-29 21:51:06 (182 MB/s) - ‘train.txt’ saved [849548/849548]\n",
            "\n",
            "--2022-09-29 21:51:07--  https://raw.githubusercontent.com/heisenberg967/ner_twitter/main/data/validation.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 103771 (101K) [text/plain]\n",
            "Saving to: ‘validation.txt’\n",
            "\n",
            "validation.txt      100%[===================>] 101.34K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2022-09-29 21:51:07 (43.8 MB/s) - ‘validation.txt’ saved [103771/103771]\n",
            "\n",
            "--2022-09-29 21:51:07--  https://raw.githubusercontent.com/heisenberg967/ner_twitter/main/data/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 106837 (104K) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>] 104.33K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-09-29 21:51:07 (75.9 MB/s) - ‘test.txt’ saved [106837/106837]\n",
            "\n",
            "--2022-09-29 21:51:07--  https://raw.githubusercontent.com/heisenberg967/ner_twitter/main/evaluation.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6112 (6.0K) [text/plain]\n",
            "Saving to: ‘evaluation.py’\n",
            "\n",
            "evaluation.py       100%[===================>]   5.97K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-09-29 21:51:08 (68.8 MB/s) - ‘evaluation.py’ saved [6112/6112]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/heisenberg967/ner_twitter/main/data/train.txt # train data\n",
        "!wget https://raw.githubusercontent.com/heisenberg967/ner_twitter/main/data/validation.txt # validation data\n",
        "!wget https://raw.githubusercontent.com/heisenberg967/ner_twitter/main/data/test.txt # test data\n",
        "!wget https://raw.githubusercontent.com/heisenberg967/ner_twitter/main/evaluation.py # used in evaluation function to calculate f-score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "492bd41a",
      "metadata": {
        "id": "492bd41a"
      },
      "source": [
        "#### 1. Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XiP6dZk7gslM",
      "metadata": {
        "id": "XiP6dZk7gslM"
      },
      "source": [
        "In this section, you're going to load the dataset and invoke the read_data function. Within the read_data function, you're going to replace all occurences of '@' with the token \\<USR\\>. You may notice how URLs in tweets are replaced with the token \\<URL\\> for your reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1115d91d",
      "metadata": {
        "id": "1115d91d"
      },
      "outputs": [],
      "source": [
        "### Function to read data\n",
        "def read_data(file_path):\n",
        "    tokens = []\n",
        "    tags = []\n",
        "    \n",
        "    tweet_tokens = []\n",
        "    tweet_tags = []\n",
        "    for line in open(file_path, encoding='utf-8'):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            if tweet_tokens:\n",
        "                tokens.append(tweet_tokens)\n",
        "                tags.append(tweet_tags)\n",
        "            tweet_tokens = []\n",
        "            tweet_tags = []\n",
        "        else:\n",
        "            token, tag = line.split()\n",
        "            \n",
        "            if token.lower().startswith('https://') or token.lower().startswith('http://'):\n",
        "                token = '<URL>'\n",
        "            # Replace all username char, i.e, '@' with <USR> token\n",
        "            ######### YOUR CODE HERE #############\n",
        "            if token.startswith('@'):\n",
        "                token = '<USR>'\n",
        "            \n",
        "            tweet_tokens.append(token)\n",
        "            tweet_tags.append(tag)\n",
        "            \n",
        "    return tokens, tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "df705d9f",
      "metadata": {
        "id": "df705d9f"
      },
      "outputs": [],
      "source": [
        "train_tokens, train_tags = read_data('train.txt')\n",
        "validation_tokens, validation_tags = read_data('validation.txt')\n",
        "test_tokens, test_tags = read_data('test.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4wN1GQgViEzX",
      "metadata": {
        "id": "4wN1GQgViEzX"
      },
      "source": [
        "Run the cell below to see what your data looks like. Feel free to play around with what you're printing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "07224733",
      "metadata": {
        "id": "07224733",
        "outputId": "65d04b88-1fcd-45bd-dbf4-5eafe27acae4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RT\tO\n",
            "<USR>\tO\n",
            ":\tO\n",
            "Online\tO\n",
            "ticket\tO\n",
            "sales\tO\n",
            "for\tO\n",
            "Ghostland\tB-musicartist\n",
            "Observatory\tI-musicartist\n",
            "extended\tO\n",
            "until\tO\n",
            "6\tO\n",
            "PM\tO\n",
            "EST\tO\n",
            "due\tO\n",
            "to\tO\n",
            "high\tO\n",
            "demand\tO\n",
            ".\tO\n",
            "Get\tO\n",
            "them\tO\n",
            "before\tO\n",
            "they\tO\n",
            "sell\tO\n",
            "out\tO\n",
            "...\tO\n",
            "\n",
            "Apple\tB-product\n",
            "MacBook\tI-product\n",
            "Pro\tI-product\n",
            "A1278\tI-product\n",
            "13.3\tI-product\n",
            "\"\tI-product\n",
            "Laptop\tI-product\n",
            "-\tI-product\n",
            "MD101LL/A\tI-product\n",
            "(\tO\n",
            "June\tO\n",
            ",\tO\n",
            "2012\tO\n",
            ")\tO\n",
            "-\tO\n",
            "Full\tO\n",
            "read\tO\n",
            "by\tO\n",
            "eBay\tB-company\n",
            "<URL>\tO\n",
            "<URL>\tO\n",
            "\n",
            "Happy\tO\n",
            "Birthday\tO\n",
            "<USR>\tO\n",
            "!\tO\n",
            "May\tO\n",
            "Allah\tB-person\n",
            "s.w.t\tO\n",
            "bless\tO\n",
            "you\tO\n",
            "with\tO\n",
            "goodness\tO\n",
            "and\tO\n",
            "happiness\tO\n",
            ".\tO\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(3):\n",
        "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
        "        print('%s\\t%s' % (token, tag))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "488a293f",
      "metadata": {
        "id": "488a293f"
      },
      "source": [
        "#### 2. Create dictionaries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wTsWCXA9i3L1",
      "metadata": {
        "id": "wTsWCXA9i3L1"
      },
      "source": [
        "In this section, you're going to be generating two dictionaries or mappings: <br>\n",
        "\\{token\\} -> \\{tokenID\\} represents row in the embedding matrix for a token <br>\n",
        "\\{tag\\} -> \\{tagID\\} one-hot encoded vector to help compute loss at the output of the network <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "EnCXR3WZd-mt",
      "metadata": {
        "id": "EnCXR3WZd-mt",
        "outputId": "ece95485-d8fd-4666-ff77-5f4f0315d7f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20503\n",
            "wakes\n",
            "21\n",
            "B-facility\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_dict(tokens_or_tags, special_tokens):\n",
        "    \"\"\"\n",
        "        tokens_or_tags: a list of lists of tokens or tags\n",
        "        special_tokens: some special tokens\n",
        "    \"\"\"\n",
        "    # Create a dictionary with default value 0\n",
        "    tok2idx = defaultdict(lambda: 0)\n",
        "    idx2tok = []\n",
        "    \n",
        "    voc = set([x for ele in tokens_or_tags for x in ele])\n",
        "    voc_size = len(voc)+len(special_tokens)\n",
        "    idx2tok = ['']*voc_size\n",
        "    \n",
        "    # Create mappings from tokens to indices and vice versa\n",
        "    # Add special tokens to dictionaries\n",
        "    # The first special token must have index 0. You may want to use voc to help create tok2idx and idx2tok mappings.\n",
        "    ######################################\n",
        "    ######### YOUR CODE HERE #############\n",
        "    ######################################\n",
        "    voc = list(voc)\n",
        "    print(len(voc))\n",
        "    print(voc[0])\n",
        "    for i in range(1,len(voc)+1):\n",
        "        print\n",
        "        tok2idx[voc[i-1]] = i\n",
        "\n",
        "    for k, v in tok2idx.items():\n",
        "        idx2tok[v] = k\n",
        "    \n",
        "\n",
        "    return tok2idx, idx2tok\n",
        "\n",
        "special_tokens = ['<UNK>', '<PAD>']\n",
        "special_tags = ['O']\n",
        "\n",
        "# Create dictionaries \n",
        "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
        "tag2idx, idx2tag = build_dict(train_tags, special_tags)\n",
        "\n",
        "# functions will help you to create the mapping between tokens and ids for a sentence\n",
        "def words2idxs(tokens_list):\n",
        "    return [token2idx[word] for word in tokens_list]\n",
        "\n",
        "def tags2idxs(tags_list):\n",
        "    return [tag2idx[tag] for tag in tags_list]\n",
        "\n",
        "def idxs2words(idxs):\n",
        "    return [idx2token[idx] for idx in idxs]\n",
        "\n",
        "def idxs2tags(idxs):\n",
        "    return [idx2tag[idx] for idx in idxs]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(idx2token[1000])\n",
        "print(token2idx['Milford'])"
      ],
      "metadata": {
        "id": "yKIS_oztGXUH",
        "outputId": "78a04887-6fcd-48b2-a797-79fac85e854f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yKIS_oztGXUH",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crocs\n",
            "1005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FkHEO6KBS_ae",
      "metadata": {
        "id": "FkHEO6KBS_ae"
      },
      "source": [
        "#### 3. Generate Batches"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_i9Mj2dflOTX",
      "metadata": {
        "id": "_i9Mj2dflOTX"
      },
      "source": [
        "The function below has been created to help train our model in batches. Now, since we want all sequences within a batch to have the same length, we're going to be padding a token *\\<PAD\\>*, as you may notice in the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "nOghuFnMeTsy",
      "metadata": {
        "id": "nOghuFnMeTsy"
      },
      "outputs": [],
      "source": [
        "# generate batches\n",
        "def batches_generator(batch_size, tokens, tags,\n",
        "                      shuffle=True, allow_smaller_last_batch=True):\n",
        "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
        "    \n",
        "    n_samples = len(tokens)\n",
        "    if shuffle:\n",
        "        order = np.random.permutation(n_samples)\n",
        "    else:\n",
        "        order = np.arange(n_samples)\n",
        "\n",
        "    n_batches = n_samples // batch_size\n",
        "    if allow_smaller_last_batch and n_samples % batch_size:\n",
        "        n_batches += 1\n",
        "\n",
        "    for k in range(n_batches):\n",
        "        batch_start = k * batch_size\n",
        "        batch_end = min((k + 1) * batch_size, n_samples)\n",
        "        current_batch_size = batch_end - batch_start\n",
        "        x_list = []\n",
        "        y_list = []\n",
        "        max_len_token = 0\n",
        "        for idx in order[batch_start: batch_end]:\n",
        "            x_list.append(words2idxs(tokens[idx]))\n",
        "            y_list.append(tags2idxs(tags[idx]))\n",
        "            max_len_token = max(max_len_token, len(tags[idx]))\n",
        "            \n",
        "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>'] # pad token to ensure equal length\n",
        "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
        "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
        "        for n in range(current_batch_size):\n",
        "            utt_len = len(x_list[n])\n",
        "            x[n, :utt_len] = x_list[n]\n",
        "            lengths[n] = utt_len\n",
        "            y[n, :utt_len] = y_list[n]\n",
        "        yield x, y, lengths"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nJViDMJBTFO2",
      "metadata": {
        "id": "nJViDMJBTFO2"
      },
      "source": [
        "#### 4. Define empty LSTM class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gizcp2ltmvqc",
      "metadata": {
        "id": "Gizcp2ltmvqc"
      },
      "source": [
        "To carry out our NER task, we're going to be building an LSTM model whose purpose is to output a probability distribution over tags for each token in a sentence. Since, we're concerned with both left and right contexts of the token, we're using a bi-directional LSTM. We're also using a dense layer to perform tag classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "wcZzHSefh2Eh",
      "metadata": {
        "id": "wcZzHSefh2Eh",
        "outputId": "2e6de3be-1ef0-4e8e-b117-dfeddeb1b1c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "### Import tf and Init LSTM Class \n",
        "\n",
        "# We're using version1 of tensorflow for this exercise.\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "\n",
        "class BiLSTMModel():\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6Tgq8m0jTJuo",
      "metadata": {
        "id": "6Tgq8m0jTJuo"
      },
      "source": [
        "#### 5. Define placeholders for network model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GNyj57cDoMfq",
      "metadata": {
        "id": "GNyj57cDoMfq"
      },
      "source": [
        "We're first going to declare the following [placeholders](https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder) to specify what data is going into the network.\n",
        " - *input_batch* — sequences of words (the shape equals to [batch_size, sequence_len]);\n",
        " - *ground_truth_tags* — sequences of tags (the shape equals to [batch_size, sequence_len]);\n",
        " - *lengths* — lengths of not padded sequences (the shape equals to [batch_size]);\n",
        " - *dropout_ph* — dropout keep probability; this placeholder has a predefined value 1;\n",
        " - *learning_rate_ph* — learning rate; we need this placeholder because we want to change the value during training.\n",
        "\n",
        "Defining shape as None lets you feed in data of variable size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "dajnItQpoF-W",
      "metadata": {
        "id": "dajnItQpoF-W"
      },
      "outputs": [],
      "source": [
        "def declare_placeholders(self):\n",
        "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
        "\n",
        "    # Placeholders for input and ground truth output.\n",
        "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
        "    self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags')\n",
        "  \n",
        "    # Placeholder for lengths of the sequences.\n",
        "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
        "    \n",
        "    # Placeholder for a dropout keep probability. If we don't feed\n",
        "    # a value for this placeholder, it will be equal to 1.0.\n",
        "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
        "    \n",
        "    # Placeholder for a learning rate (tf.float32).\n",
        "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name='learning_rate')\n",
        "\n",
        "BiLSTMModel.__declare_placeholders = classmethod(declare_placeholders)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lSfDMKmBpBcG",
      "metadata": {
        "id": "lSfDMKmBpBcG"
      },
      "source": [
        "#### 6. Define Layers of the network model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "xHspjLArqf2_",
      "metadata": {
        "id": "xHspjLArqf2_"
      },
      "outputs": [],
      "source": [
        "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
        "    \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
        "    \n",
        "    # Create embedding variable (tf.Variable) with dtype tf.float32\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/compat/v1/Variable\n",
        "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
        "    embedding_matrix_variable = tf.Variable(initial_value=tf.convert_to_tensor(initial_embedding_matrix, dtype=tf.float32), name='embedding_matrix', dtype=tf.float32) ######### Replace None with the intended value for initial_value #############\n",
        "    \n",
        "    # Create RNN cells (for example, tf.nn.rnn_cell.BasicLSTMCell) with n_hidden_rnn number of units \n",
        "    # and dropout (tf.nn.rnn_cell.DropoutWrapper), initializing all *_keep_prob with dropout placeholder.\n",
        "    forward_cell =  tf.nn.rnn_cell.DropoutWrapper(\n",
        "        tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn), input_keep_prob=self.dropout_ph, output_keep_prob=self.dropout_ph, state_keep_prob=self.dropout_ph)\n",
        "    backward_cell =  tf.nn.rnn_cell.DropoutWrapper(\n",
        "        tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn), input_keep_prob=self.dropout_ph, output_keep_prob=self.dropout_ph, state_keep_prob=self.dropout_ph)\n",
        "\n",
        "    # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
        "    # Shape: [batch_size, sequence_len, embedding_dim].\n",
        "    embeddings =  tf.nn.embedding_lookup(embedding_matrix_variable, self.input_batch)\n",
        "    \n",
        "    # Pass them through Bidirectional Dynamic RNN (tf.nn.bidirectional_dynamic_rnn).\n",
        "    # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn]. \n",
        "    # Also don't forget to initialize sequence_length as self.lengths and dtype as tf.float32.\n",
        "    (rnn_output_fw, rnn_output_bw), _ =  tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, embeddings, self.lengths, dtype=tf.float32)\n",
        "    rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
        "\n",
        "    # Dense layer on top.\n",
        "    # Shape: [batch_size, sequence_len, n_tags].   \n",
        "    self.logits = tf.layers.dense(rnn_output, n_tags, activation=None)\n",
        "\n",
        "BiLSTMModel.__build_layers = classmethod(build_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JRR23hpXqjC2",
      "metadata": {
        "id": "JRR23hpXqjC2"
      },
      "source": [
        "#### 7. Compute Predictions and Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T_gXlrpNr8RV",
      "metadata": {
        "id": "T_gXlrpNr8RV"
      },
      "source": [
        "In this section, we're going to apply softmax to the last layer and use argmax to determine the most probable tags as defined in the function compute_predictions. <br>\n",
        "The compute_loss function is used to create our loss function for which we're making use of tensorflow's [cross entropy with logits](https://www.tensorflow.org/api_docs/python/tf/compat/v1/nn/softmax_cross_entropy_with_logits_v2). Also, we're going to be ignoring loss coming from the \\<PAD\\> tokens we'd created earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "uXxz7XTDtxsX",
      "metadata": {
        "id": "uXxz7XTDtxsX"
      },
      "outputs": [],
      "source": [
        "def compute_predictions(self):\n",
        "    \"\"\"Transforms logits to probabilities and finds the most probable tags.\"\"\"\n",
        "    \n",
        "    # Create softmax (tf.nn.softmax) function\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/compat/v1/math/softmax\n",
        "    # Your task is to create a softmax function and assign it to softmax_output.\n",
        "    # Replace None with intended value to invoke tf.nn.softmax\n",
        "    # Remember, you'd defined your logits value earlier in build_layers\n",
        "    softmax_output = tf.compat.v1.math.softmax(self.logits) ######### YOUR CODE HERE #############\n",
        "    \n",
        "    # Use argmax (tf.argmax) to get the most probable tags with axis=-1\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/compat/v1/argmax\n",
        "    # Replace None with intended value to invoke tf.argmax\n",
        "    self.predictions = tf.argmax(softmax_output, axis=-1) ######### YOUR CODE HERE #############\n",
        "\n",
        "BiLSTMModel.__compute_predictions = classmethod(compute_predictions)\n",
        "\n",
        "def compute_loss(self, n_tags, PAD_index):\n",
        "    \"\"\"Computes masked cross-entopy loss with logits.\"\"\"\n",
        "    \n",
        "    # Create cross entropy function function (tf.nn.softmax_cross_entropy_with_logits)\n",
        "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
        "    loss_tensor =  tf.nn.softmax_cross_entropy_with_logits_v2(labels=ground_truth_tags_one_hot, logits=self.logits)\n",
        "    \n",
        "    # Create loss function which doesn't operate with <PAD> tokens (tf.reduce_mean)\n",
        "    mask = tf.cast(tf.not_equal(loss_tensor, PAD_index), tf.float32)\n",
        "    self.loss =  tf.reduce_mean(tf.multiply(loss_tensor, mask))\n",
        "\n",
        "BiLSTMModel.__compute_loss = classmethod(compute_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JrUKrBqRt0b5",
      "metadata": {
        "id": "JrUKrBqRt0b5"
      },
      "source": [
        "#### 8. Optimize loss using Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1HeIX1UAeXuU",
      "metadata": {
        "id": "1HeIX1UAeXuU"
      },
      "outputs": [],
      "source": [
        "def perform_optimization(self):\n",
        "    \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
        "    \n",
        "    # Your task is to create an optimizer (tf.train.AdamOptimizer)\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/AdamOptimizer\n",
        "    # replace None with tf.train.AdamOptimizer\n",
        "    self.optimizer =  tf.train.AdamOptimizer() ######### YOUR CODE HERE #############\n",
        "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
        "    \n",
        "    # Gradient clipping (tf.clip_by_norm) for self.grads_and_vars\n",
        "    # Pay attention that you need to apply this operation only for gradients \n",
        "    # because self.grads_and_vars contains also variables.\n",
        "    # list comprehension mught be useful in this case.\n",
        "    clip_norm = tf.cast(1.0, tf.float32)\n",
        "    self.grads_and_vars =[ (tf.clip_by_norm(x[0], clip_norm),x[1]) for x in self.grads_and_vars]\n",
        "    \n",
        "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)\n",
        "\n",
        "BiLSTMModel.__perform_optimization = classmethod(perform_optimization)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jI3Z8Ce3TNrf",
      "metadata": {
        "id": "jI3Z8Ce3TNrf"
      },
      "source": [
        "#### 9. Build LSTM class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_YNcrb6buYlE",
      "metadata": {
        "id": "_YNcrb6buYlE"
      },
      "source": [
        "Great! So, we've defined all the components in our network model, so we pass on the functions onto our LSTM class' constructor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "mrStfaW7eXqJ",
      "metadata": {
        "id": "mrStfaW7eXqJ"
      },
      "outputs": [],
      "source": [
        "## Build LSTM class\n",
        "\n",
        "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
        "    self.__declare_placeholders()\n",
        "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
        "    self.__compute_predictions()\n",
        "    self.__compute_loss(n_tags, PAD_index)\n",
        "    self.__perform_optimization()\n",
        "\n",
        "BiLSTMModel.__init__ = classmethod(init_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JMrFzaZHTQck",
      "metadata": {
        "id": "JMrFzaZHTQck"
      },
      "source": [
        "#### 10. Train Neural Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZCttOatKu8ro",
      "metadata": {
        "id": "ZCttOatKu8ro"
      },
      "source": [
        "In order to train the network model that we've built, we're going to be computing *self.train_op* that we'd declared within the *perform_optimization* function. We're going to feed our actual data into the placeholders we'd defined as you may observe in *feed_dict* "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "AlDnOwFXeXiE",
      "metadata": {
        "id": "AlDnOwFXeXiE"
      },
      "outputs": [],
      "source": [
        "## TRAIN NN\n",
        "\n",
        "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
        "    feed_dict = {self.input_batch: x_batch,\n",
        "                 self.ground_truth_tags: y_batch,\n",
        "                 self.learning_rate_ph: learning_rate,\n",
        "                 self.dropout_ph: dropout_keep_probability,\n",
        "                 self.lengths: lengths}\n",
        "    \n",
        "    # https://www.tensorflow.org/api_docs/python/tf/compat/v1/Session\n",
        "    session.run(self.train_op, feed_dict=feed_dict)\n",
        "\n",
        "BiLSTMModel.train_on_batch = classmethod(train_on_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bjK3LNrgTTXi",
      "metadata": {
        "id": "bjK3LNrgTTXi"
      },
      "source": [
        "#### 11. Generate tag predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y37ITXpMwtm5",
      "metadata": {
        "id": "Y37ITXpMwtm5"
      },
      "source": [
        "To predict tags, we're going to compute self.predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "sYTzxfa7eXQY",
      "metadata": {
        "id": "sYTzxfa7eXQY"
      },
      "outputs": [],
      "source": [
        "## PREDICT\n",
        "\n",
        "def predict_for_batch(self, session, x_batch, lengths):    \n",
        "    feed_dict = {self.input_batch: x_batch,\n",
        "                 self.dropout_ph: 1.0,\n",
        "                 self.lengths: lengths}\n",
        "\n",
        "    predictions = session.run(self.predictions, feed_dict=feed_dict)\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "BiLSTMModel.predict_for_batch = classmethod(predict_for_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67XcagI9TbRz",
      "metadata": {
        "id": "67XcagI9TbRz"
      },
      "source": [
        "#### 12. Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4WaxZHNxEg5",
      "metadata": {
        "id": "a4WaxZHNxEg5"
      },
      "source": [
        "To help in evaluating our model, we're going to create two functions. <br>\n",
        "The *predict_tags* gets predictions from a network model and then transforms indices to tokens and tags. <br>\n",
        "The *eval_conll* function calculates precision, recall and F1 score. <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "r2EJPs7keUNf",
      "metadata": {
        "id": "r2EJPs7keUNf"
      },
      "outputs": [],
      "source": [
        "## Evaluate model \n",
        "\n",
        "from evaluation import precision_recall_f1\n",
        "\n",
        "def predict_tags(model, session, token_idxs_batch, lengths):\n",
        "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
        "    \n",
        "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
        "    \n",
        "    tags_batch, tokens_batch = [], []\n",
        "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
        "        tags, tokens = [], []\n",
        "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
        "            tags.append(idx2tag[tag_idx])\n",
        "            tokens.append(idx2token[token_idx])\n",
        "        tags_batch.append(tags)\n",
        "        tokens_batch.append(tokens)\n",
        "    return tags_batch, tokens_batch\n",
        "    \n",
        "    \n",
        "def eval_conll(model, session, tokens, tags, short_report=True):\n",
        "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
        "    \n",
        "    y_true, y_pred = [], []\n",
        "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
        "        tags_batch, tokens_batch = predict_tags(model, session, x_batch, lengths)\n",
        "        if len(x_batch[0]) != len(tags_batch[0]):\n",
        "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
        "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
        "        predicted_tags = []\n",
        "        ground_truth_tags = []\n",
        "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
        "            if token != '<PAD>':\n",
        "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
        "                predicted_tags.append(pred_tag)\n",
        "\n",
        "        # We extend every prediction and ground truth sequence with 'O' tag to indicate a possible end of entity.\n",
        "        y_true.extend(ground_truth_tags + ['O'])\n",
        "        y_pred.extend(predicted_tags + ['O'])\n",
        "        \n",
        "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wkmkcX8SThQ3",
      "metadata": {
        "id": "wkmkcX8SThQ3"
      },
      "source": [
        "#### 13. Set/Adjust hyperparameters for BiLSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SwP7PAmKx2_j",
      "metadata": {
        "id": "SwP7PAmKx2_j"
      },
      "source": [
        "So, in this section, you're going to see all the different pieces come together. <br>\n",
        "\n",
        "Our *BiLSTMModel* model has the following parameters:\n",
        " - *vocabulary_size* — number of tokens;\n",
        " - *n_tags* — number of tags;\n",
        " - *embedding_dim* — dimension of embeddings, recommended value: 200;\n",
        " - *n_hidden_rnn* — size of hidden layers for RNN, recommended value: 200;\n",
        " - *PAD_index* — an index of the padding token (\\<PAD\\>).\n",
        "\n",
        "Run your model by setting different hyperparameters. You could start with the following values\n",
        "- *batch_size*: 32; (alternatively 8-128)\n",
        "- 4 epochs; (In the interest of time, you may want to limit it to 8)\n",
        "- starting value of *learning_rate*: 0.005 (between 0.001 and 0.1)\n",
        "- *learning_rate_decay*: a square root of 2;\n",
        "- *dropout_keep_probability*: try several values: 0.1, 0.5, 0.9.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7Uckl7Fofvwo",
      "metadata": {
        "id": "7Uckl7Fofvwo",
        "outputId": "3c9cfb5f-3f88-4976-f96f-4c271680273e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  \n",
            "WARNING:tensorflow:From <ipython-input-10-9623c57adece>:23: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/rnn.py:446: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:756: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  shape=[input_depth + h_depth, 4 * self._num_units])\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:760: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:760: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=tf.compat.v1.zeros_initializer(dtype=self.dtype))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n"
          ]
        }
      ],
      "source": [
        "### RUN NN by setting different hyperparameters\n",
        "### BUILD the model\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "model = BiLSTMModel(vocabulary_size=len(token2idx), n_tags=len(tag2idx), embedding_dim=200, n_hidden_rnn=200, PAD_index=token2idx['<PAD>'])\n",
        "\n",
        "batch_size = 32 ######### YOUR CODE HERE #############\n",
        "n_epochs = 10 ######### YOUR CODE HERE #############\n",
        "learning_rate = 0.001 ######### YOUR CODE HERE #############\n",
        "learning_rate_decay = 1.4 ######### YOUR CODE HERE #############\n",
        "dropout_keep_probability = 0.5 ######### YOUR CODE HERE #############"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LeBzw5A0Tr3Q",
      "metadata": {
        "id": "LeBzw5A0Tr3Q"
      },
      "source": [
        "#### 14. Run your model with the train/validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aEm6xfyTy_GW",
      "metadata": {
        "id": "aEm6xfyTy_GW"
      },
      "source": [
        "Okay! We're now ready to run our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "hpcXm_FZfvuC",
      "metadata": {
        "id": "hpcXm_FZfvuC",
        "outputId": "51e87abc-9546-4ed1-ddbe-347a87300c99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training... \n",
            "\n",
            "-------------------- Epoch 1 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 73071 phrases; correct: 170.\n",
            "\n",
            "precision:  0.23%; recall:  3.79%; F1:  0.44\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 8719 phrases; correct: 18.\n",
            "\n",
            "precision:  0.21%; recall:  3.35%; F1:  0.39\n",
            "\n",
            "-------------------- Epoch 2 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 0 phrases; correct: 0.\n",
            "\n",
            "precision:  0.00%; recall:  0.00%; F1:  0.00\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 0 phrases; correct: 0.\n",
            "\n",
            "precision:  0.00%; recall:  0.00%; F1:  0.00\n",
            "\n",
            "-------------------- Epoch 3 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 2372 phrases; correct: 389.\n",
            "\n",
            "precision:  16.40%; recall:  8.67%; F1:  11.34\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 229 phrases; correct: 38.\n",
            "\n",
            "precision:  16.59%; recall:  7.08%; F1:  9.92\n",
            "\n",
            "-------------------- Epoch 4 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4583 phrases; correct: 703.\n",
            "\n",
            "precision:  15.34%; recall:  15.66%; F1:  15.50\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 330 phrases; correct: 49.\n",
            "\n",
            "precision:  14.85%; recall:  9.12%; F1:  11.30\n",
            "\n",
            "-------------------- Epoch 5 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4927 phrases; correct: 1373.\n",
            "\n",
            "precision:  27.87%; recall:  30.59%; F1:  29.16\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 399 phrases; correct: 86.\n",
            "\n",
            "precision:  21.55%; recall:  16.01%; F1:  18.38\n",
            "\n",
            "-------------------- Epoch 6 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4371 phrases; correct: 1633.\n",
            "\n",
            "precision:  37.36%; recall:  36.38%; F1:  36.86\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 325 phrases; correct: 104.\n",
            "\n",
            "precision:  32.00%; recall:  19.37%; F1:  24.13\n",
            "\n",
            "-------------------- Epoch 7 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4524 phrases; correct: 2481.\n",
            "\n",
            "precision:  54.84%; recall:  55.27%; F1:  55.05\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 351 phrases; correct: 145.\n",
            "\n",
            "precision:  41.31%; recall:  27.00%; F1:  32.66\n",
            "\n",
            "-------------------- Epoch 8 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4591 phrases; correct: 3004.\n",
            "\n",
            "precision:  65.43%; recall:  66.92%; F1:  66.17\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 373 phrases; correct: 162.\n",
            "\n",
            "precision:  43.43%; recall:  30.17%; F1:  35.60\n",
            "\n",
            "-------------------- Epoch 9 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4646 phrases; correct: 3494.\n",
            "\n",
            "precision:  75.20%; recall:  77.83%; F1:  76.50\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 377 phrases; correct: 169.\n",
            "\n",
            "precision:  44.83%; recall:  31.47%; F1:  36.98\n",
            "\n",
            "-------------------- Epoch 10 of 10 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4738 phrases; correct: 3782.\n",
            "\n",
            "precision:  79.82%; recall:  84.25%; F1:  81.98\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 420 phrases; correct: 190.\n",
            "\n",
            "precision:  45.24%; recall:  35.38%; F1:  39.71\n",
            "\n",
            "...training finished.\n"
          ]
        }
      ],
      "source": [
        "### Train model using dataset\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Start training... \\n')\n",
        "for epoch in range(n_epochs):\n",
        "    # For each epoch evaluate the model on train and validation data\n",
        "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
        "    print('Train data evaluation:')\n",
        "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
        "    print('Validation data evaluation:')\n",
        "    eval_conll(model, sess, validation_tokens, validation_tags, short_report=True)\n",
        "    \n",
        "    # Train the model\n",
        "    for x_batch, y_batch, lengths in batches_generator(batch_size, train_tokens, train_tags):\n",
        "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
        "        \n",
        "    # Decaying the learning rate\n",
        "    learning_rate = learning_rate / learning_rate_decay\n",
        "    \n",
        "print('...training finished.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-gPPuEOXTwWw",
      "metadata": {
        "id": "-gPPuEOXTwWw"
      },
      "source": [
        "#### 15. Look at Results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-AQoCTXmz5NZ",
      "metadata": {
        "id": "-AQoCTXmz5NZ"
      },
      "source": [
        "Congrats on making it till the end! You may observe your results by running the cell below. If you did things right, the F1 score on your validation set should be close to 40%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "QfxH51CEfvrK",
      "metadata": {
        "id": "QfxH51CEfvrK",
        "outputId": "f236c7c7-93b4-49d6-d5e0-37726207b54e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- Train set quality: --------------------\n",
            "processed 105778 tokens with 4489 phrases; found: 4536 phrases; correct: 3968.\n",
            "\n",
            "precision:  87.48%; recall:  88.39%; F1:  87.93\n",
            "\n",
            "\t     company: precision:   91.79%; recall:   93.93%; F1:   92.85; predicted:   658\n",
            "\n",
            "\t    facility: precision:   86.80%; recall:   94.27%; F1:   90.38; predicted:   341\n",
            "\n",
            "\t     geo-loc: precision:   93.62%; recall:   97.29%; F1:   95.42; predicted:  1035\n",
            "\n",
            "\t       movie: precision:   72.22%; recall:   57.35%; F1:   63.93; predicted:    54\n",
            "\n",
            "\t musicartist: precision:   72.65%; recall:   69.83%; F1:   71.21; predicted:   223\n",
            "\n",
            "\t       other: precision:   79.98%; recall:   88.11%; F1:   83.85; predicted:   834\n",
            "\n",
            "\t      person: precision:   92.19%; recall:   94.58%; F1:   93.37; predicted:   909\n",
            "\n",
            "\t     product: precision:   83.33%; recall:   75.47%; F1:   79.21; predicted:   288\n",
            "\n",
            "\t  sportsteam: precision:   81.62%; recall:   69.59%; F1:   75.12; predicted:   185\n",
            "\n",
            "\t      tvshow: precision:   22.22%; recall:    3.45%; F1:    5.97; predicted:     9\n",
            "\n",
            "-------------------- Validation set quality: --------------------\n",
            "processed 12836 tokens with 537 phrases; found: 396 phrases; correct: 185.\n",
            "\n",
            "precision:  46.72%; recall:  34.45%; F1:  39.66\n",
            "\n",
            "\t     company: precision:   65.12%; recall:   53.85%; F1:   58.95; predicted:    86\n",
            "\n",
            "\t    facility: precision:   37.14%; recall:   38.24%; F1:   37.68; predicted:    35\n",
            "\n",
            "\t     geo-loc: precision:   68.75%; recall:   48.67%; F1:   56.99; predicted:    80\n",
            "\n",
            "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     2\n",
            "\n",
            "\t musicartist: precision:   44.44%; recall:   14.29%; F1:   21.62; predicted:     9\n",
            "\n",
            "\t       other: precision:   26.17%; recall:   34.57%; F1:   29.79; predicted:   107\n",
            "\n",
            "\t      person: precision:   44.44%; recall:   21.43%; F1:   28.92; predicted:    54\n",
            "\n",
            "\t     product: precision:   13.33%; recall:    5.88%; F1:    8.16; predicted:    15\n",
            "\n",
            "\t  sportsteam: precision:   37.50%; recall:   15.00%; F1:   21.43; predicted:     8\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n",
            "-------------------- Test set quality: --------------------\n",
            "processed 13258 tokens with 604 phrases; found: 563 phrases; correct: 239.\n",
            "\n",
            "precision:  42.45%; recall:  39.57%; F1:  40.96\n",
            "\n",
            "\t     company: precision:   61.82%; recall:   40.48%; F1:   48.92; predicted:    55\n",
            "\n",
            "\t    facility: precision:   35.59%; recall:   44.68%; F1:   39.62; predicted:    59\n",
            "\n",
            "\t     geo-loc: precision:   65.03%; recall:   56.36%; F1:   60.39; predicted:   143\n",
            "\n",
            "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     1\n",
            "\n",
            "\t musicartist: precision:   20.00%; recall:    7.41%; F1:   10.81; predicted:    10\n",
            "\n",
            "\t       other: precision:   28.70%; recall:   32.04%; F1:   30.28; predicted:   115\n",
            "\n",
            "\t      person: precision:   36.09%; recall:   46.15%; F1:   40.51; predicted:   133\n",
            "\n",
            "\t     product: precision:    4.35%; recall:    3.57%; F1:    3.92; predicted:    23\n",
            "\n",
            "\t  sportsteam: precision:   29.17%; recall:   22.58%; F1:   25.45; predicted:    24\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## RESULT\n",
        "\n",
        "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
        "train_results = eval_conll(model, sess, train_tokens, train_tags, short_report=False)\n",
        "\n",
        "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
        "validation_results = eval_conll(model, sess, validation_tokens, validation_tags, short_report=False)\n",
        "\n",
        "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
        "test_results = eval_conll(model, sess, test_tokens, test_tags, short_report=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2c436508",
        "492bd41a",
        "488a293f",
        "FkHEO6KBS_ae",
        "nJViDMJBTFO2",
        "6Tgq8m0jTJuo",
        "lSfDMKmBpBcG",
        "JRR23hpXqjC2",
        "JrUKrBqRt0b5",
        "jI3Z8Ce3TNrf",
        "JMrFzaZHTQck",
        "bjK3LNrgTTXi",
        "67XcagI9TbRz",
        "wkmkcX8SThQ3",
        "LeBzw5A0Tr3Q",
        "-gPPuEOXTwWw"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}