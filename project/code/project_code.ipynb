{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Libraries and overhead***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5HXuKV4LFLfH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/biocros/marn/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import F1Score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "BBLaGu1SbRMP",
    "outputId": "267e5d50-154b-4156-b904-d2f6268533a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1+cu102'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W37VfQuqmms8",
    "outputId": "f8052a67-9e17-4fae-c576-1975052cad7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = ['а','б','в','г','д','е','ё','ж','з','и','й','к','л','м','н','о','п','р','с','т','у',\n",
    "            'ф','х','ц','ч','ш','щ','ъ','ы','ь','э','ю','я']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Helper Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ortho(data, orthographies):\n",
    "    newdata = []\n",
    "    for line in data:\n",
    "        line = line.lower()\n",
    "        newline = ''\n",
    "        r = np.random.randint(len(orthographies))\n",
    "        for char in line:\n",
    "            if orthographies[r].get(char):\n",
    "                newline += orthographies[r].get(char)\n",
    "            else:\n",
    "                newline += char\n",
    "        \n",
    "        newdata.append(newline)\n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readin_orthos():\n",
    "    with open('../data/orthographies.txt') as f:\n",
    "        rawtext = f.readlines()\n",
    "    orthos = []\n",
    "    for line in rawtext:\n",
    "        if (line[0] == '#'):\n",
    "            tmp = defaultdict();\n",
    "        else:\n",
    "            line = line.rstrip()\n",
    "            sp = line.split('\\t')\n",
    "            tmp.update({sp[0] : sp[1]})\n",
    "        if (len(tmp) == 33):\n",
    "            orthos.append(tmp)\n",
    "\n",
    "\n",
    "    return orthos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ortho_data(data):\n",
    "    cdata = []\n",
    "    for entry in data:\n",
    "        if len(entry) == 2:\n",
    "            continue\n",
    "        e = re.sub(r'[^\\w]','', entry)\n",
    "        e = e.lower()\n",
    "        e = list(e)\n",
    "        e.insert(0, '<STR>')\n",
    "        e.append('<END>')\n",
    "        cdata.append(e)\n",
    "    \n",
    "    return cdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    cdata = []\n",
    "    for entry in data:\n",
    "        if len(entry) == 2:\n",
    "            continue\n",
    "        e = re.sub(r'[^\\w]','', entry)\n",
    "        e = e.lower()\n",
    "        if re.sub(r'[бвгджзклмнпрстфхцчшщаэыуояеёюиьъй]', '', e):\n",
    "            continue\n",
    "        e = list(e)\n",
    "        e.insert(0, '<STR>')\n",
    "        e.append('<END>')\n",
    "        cdata.append(e)\n",
    "    \n",
    "    return cdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(data):\n",
    "    labels = []\n",
    "    for entry in data:\n",
    "        tmp = []\n",
    "        for i in range(len(entry) - 1):\n",
    "            tmp.append(entry[i + 1])\n",
    "        labels.append(tmp)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, text, labels):\n",
    "        self.labels = labels\n",
    "        self.text = text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        text = self.text[idx]\n",
    "        sample = (text, label)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the ultimate character to the vector\n",
    "def convert_label_to_vec(data, dicti):\n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        tmp_list = np.zeros(len(dicti))\n",
    "        # tmp_list = [0 for j in range(len(dicti))]\n",
    "        tmp_list[dicti[data[i]] - 1] = 1\n",
    "        new_data.append(tmp_list)\n",
    "\n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the penultimate character to the vector\n",
    "def pair_vecs(data, vecs, dicti):\n",
    "    vecs[0] *= 2\n",
    "    for i in range(1, len(data)):\n",
    "        vecs[i][dicti[data[i - 1]] - 1] += 1\n",
    "    \n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyr_cyr_step(data):\n",
    "    for line in data:\n",
    "        line.pop()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"\n",
    "    After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    f1_weighted = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for _, (text, labels) in enumerate(tqdm(val_dataloader, position = 0, leave = True)):\n",
    "        # Load batch to GPU\n",
    "        text = text.type(torch.FloatTensor).to(device)\n",
    "        labels = labels.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            output = model(text)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(output)\n",
    "        labelloc = torch.argmax(labels)\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == labelloc).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "    f1_weighted = np.mean(f1_weighted)\n",
    "\n",
    "    return val_loss, val_accuracy, f1_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model declarations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Probability Model (Cyrillic->Cyrillic prediction)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PModel():\n",
    "    def __init__(self, alphabet):\n",
    "        self.P = defaultdict()\n",
    "        for char in alphabet:\n",
    "            self.P.update({char : defaultdict()})\n",
    "        \n",
    "        for key, _ in self.P.items():\n",
    "            for char in alphabet:\n",
    "                self.P[key].update({char : 0})\n",
    "\n",
    "    def display_model(self):\n",
    "        for key, value in self.P.items():\n",
    "            print('P[' + key + '] = ' + str(value))\n",
    "            # for keyo, val in self.P[key].items():\n",
    "            #     print('P[' + key + '][' + keyo + '] = ' + str(val), end = ' ')\n",
    "            # print('\\n')\n",
    "    \n",
    "    def model_build(self, X, Y):\n",
    "        total = len(X)\n",
    "        total_pairs = len(X) - 1\n",
    "        indv_chars = defaultdict(lambda : False)\n",
    "\n",
    "        for i, (cur, nex) in enumerate(zip(X, Y)):\n",
    "            self.P[cur][nex] += 1\n",
    "\n",
    "            if indv_chars[cur]:\n",
    "                indv_chars[cur] += 1\n",
    "            else:\n",
    "                indv_chars.update({cur : 1})\n",
    "        indv_chars['<END>'] += 1 # off by 1 error since it's not at the end of X\n",
    "\n",
    "        for key, _ in indv_chars.items():\n",
    "            indv_chars[key] /= float(total)\n",
    "\n",
    "        for (cur, nex) in set(zip(X, Y)):\n",
    "            self.P[cur][nex] = float(self.P[cur][nex]) / (total_pairs)\n",
    "            self.P[cur][nex] = self.P[cur][nex] / indv_chars[cur]\n",
    "    \n",
    "    def model_test(self, X, Y):\n",
    "        correct = 0\n",
    "        total = len(X)\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            pred = self.predict_next(X[i])\n",
    "            if pred == Y[i]:\n",
    "                correct += 1\n",
    "        \n",
    "        return float(correct) / total\n",
    "        \n",
    "    def predict_next(self, char):\n",
    "        return max(self.P[char], key=self.P[char].get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Feedforward Model (Cyrillic->Cyrillic prediction)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "     def __init__(self, input_size, hidden_size, num_classes):\n",
    "         super(NeuralNet, self).__init__()\n",
    "         self.input_size = input_size\n",
    "         self.l1 = nn.Linear(input_size, hidden_size) \n",
    "         self.relu = nn.ReLU()\n",
    "         self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "         self.s1 = nn.Softmax(dim = 0)\n",
    "\n",
    "     def forward(self, x):\n",
    "         out = self.l1(x)\n",
    "         out = self.relu(out)\n",
    "         out = self.l2(out)\n",
    "         out = self.s1(out)\n",
    "         return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Transducer Model (Latin->Cyrillic prediction)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Joiner(torch.nn.Module):\n",
    "#   def __init__(self, num_outputs):\n",
    "#     super(Joiner, self).__init__()\n",
    "#     self.linear = torch.nn.Linear(joiner_dim, num_outputs)\n",
    "\n",
    "#   def forward(self, encoder_out, predictor_out):\n",
    "#     out = encoder_out + predictor_out\n",
    "#     out = torch.nn.functional.relu(out)\n",
    "#     out = self.linear(out)\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(torch.nn.Module):\n",
    "#   def __init__(self, num_inputs):\n",
    "#     super(Encoder, self).__init__()\n",
    "#     self.embed = torch.nn.Embedding(num_inputs, encoder_dim)\n",
    "#     self.rnn = torch.nn.GRU(input_size=encoder_dim, hidden_size=encoder_dim, num_layers=3, batch_first=True, bidirectional=True, dropout=0.1)\n",
    "#     self.linear = torch.nn.Linear(encoder_dim*2, joiner_dim)\n",
    "\n",
    "#   def forward(self, x):\n",
    "#     out = x\n",
    "#     out = self.embed(out)\n",
    "#     out = self.rnn(out)[0]\n",
    "#     out = self.linear(out)\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Predictor(torch.nn.Module):\n",
    "#   def __init__(self, num_outputs):\n",
    "#     super(Predictor, self).__init__()\n",
    "#     self.embed = torch.nn.Embedding(num_outputs, predictor_dim)\n",
    "#     self.rnn = torch.nn.GRUCell(input_size=predictor_dim, hidden_size=predictor_dim)\n",
    "#     self.linear = torch.nn.Linear(predictor_dim, joiner_dim)\n",
    "    \n",
    "#     self.initial_state = torch.nn.Parameter(torch.randn(predictor_dim))\n",
    "#     self.start_symbol = NULL_INDEX # In the original paper, a vector of 0s is used; just using the null index instead is easier when using an Embedding layer.\n",
    "\n",
    "#   def forward_one_step(self, input, previous_state):\n",
    "#     embedding = self.embed(input)\n",
    "#     state = self.rnn.forward(embedding, previous_state)\n",
    "#     out = self.linear(state)\n",
    "#     return out, state\n",
    "\n",
    "#   def forward(self, y):\n",
    "#     batch_size = y.shape[0]\n",
    "#     U = y.shape[1]\n",
    "#     outs = []\n",
    "#     state = torch.stack([self.initial_state] * batch_size).to(y.device)\n",
    "#     for u in range(U+1): # need U+1 to get null output for final timestep \n",
    "#       if u == 0:\n",
    "#         decoder_input = torch.tensor([self.start_symbol] * batch_size).to(y.device)\n",
    "#       else:\n",
    "#         decoder_input = y[:,u-1]\n",
    "#       out, state = self.forward_one_step(decoder_input, state)\n",
    "#       outs.append(out)\n",
    "#     out = torch.stack(outs, dim=1)\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transducer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, beam_len,\n",
    "                 encoder = 0,\n",
    "                 joiner = 0,\n",
    "                 predictor = 0):\n",
    "        super(Transducer, self).__init__()\n",
    "        if encoder:\n",
    "            self.encoder = encoder\n",
    "        else:\n",
    "            self.encoder = Encoder(input_size, output_size)\n",
    "        \n",
    "        if joiner:\n",
    "            self.joiner = joiner\n",
    "        else:\n",
    "            self.joiner = Joiner(input_size, output_size)\n",
    "\n",
    "        if predictor:\n",
    "            self.predictor = predictor\n",
    "        else:\n",
    "            self.predictor = Predictor(input_size, output_size)\n",
    "\n",
    "    self.beam_len = beam_len\n",
    "    \n",
    "    # take one step through the network to get a final softmax\n",
    "    def network_step(self, y):\n",
    "        sftmx = self.predictor.forward(y)\n",
    "        embed = self.encoder.forward(y)\n",
    "        out = self.joiner.forward(embed, sftmx)\n",
    "        return out\n",
    "        \n",
    "    # take one step through a full example with multiple network steps\n",
    "    def forward(self, y):\n",
    "        batch_size = y.shape[0]\n",
    "        U = y.shape[1]\n",
    "        \n",
    "        preds = []\n",
    "        best = []\n",
    "        \n",
    "        for u in range(U + 1):\n",
    "            if (u == 0):\n",
    "                prev = '<STR>'\n",
    "            else:\n",
    "                prev = y[:, u - 1]\n",
    "            \n",
    "            out = self.network_step(self, y)\n",
    "            preds.append(torch.max(out))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Data preprocessing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/words.txt') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "orthographies = readin_orthos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho_data = convert_to_ortho(data, orthographies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortho_data = clean_ortho_data(ortho_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "T9z2bCjnxliG"
   },
   "outputs": [],
   "source": [
    "cdata = clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "x7GJr9Yd3zZy"
   },
   "outputs": [],
   "source": [
    "labels = get_labels(cdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdata = cyr_cyr_step(cdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "AhT7TFFV6LPf"
   },
   "outputs": [],
   "source": [
    "X = [char for word in cdata for char in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Vq0LCLJH6omv"
   },
   "outputs": [],
   "source": [
    "Y = [char for target in labels for char in target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Feed forward model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "0XUpPpLAJG6W"
   },
   "outputs": [],
   "source": [
    "alpha_dict = defaultdict()\n",
    "for i in range(len(ALPHABET)):\n",
    "    alpha_dict.update({ALPHABET[i]: i + 1})\n",
    "\n",
    "alpha_dict.update({'<STR>' : len(ALPHABET) + 1})\n",
    "alpha_dict.update({'<END>' : len(ALPHABET) + 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7FbXCeFJ1iE",
    "outputId": "f28d947b-7c53-4b2f-c4ea-2212edb7532b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'а': 1, 'б': 2, 'в': 3, 'г': 4, 'д': 5, 'е': 6, 'ё': 7, 'ж': 8, 'з': 9, 'и': 10, 'й': 11, 'к': 12, 'л': 13, 'м': 14, 'н': 15, 'о': 16, 'п': 17, 'р': 18, 'с': 19, 'т': 20, 'у': 21, 'ф': 22, 'х': 23, 'ц': 24, 'ч': 25, 'ш': 26, 'щ': 27, 'ъ': 28, 'ы': 29, 'ь': 30, 'э': 31, 'ю': 32, 'я': 33, '<STR>': 34, '<END>': 35})\n"
     ]
    }
   ],
   "source": [
    "print(alpha_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "h_lX3zC1J7Pu"
   },
   "outputs": [],
   "source": [
    "numX = convert_label_to_vec(X, alpha_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "s8m13WiQt7B_"
   },
   "outputs": [],
   "source": [
    "# numX = pair_vecs(X, numX, alpha_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "QQYnpGXaPvo-"
   },
   "outputs": [],
   "source": [
    "numY = convert_label_to_vec(Y, alpha_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "C-PYP3iwVDiW"
   },
   "outputs": [],
   "source": [
    "numX = torch.Tensor(numX).to(device = device, dtype=torch.float)\n",
    "numY = torch.Tensor(numY).to(device = device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numX[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "w5bpZgE_6mV8"
   },
   "outputs": [],
   "source": [
    "data_obj = CustomTextDataset(numX, numY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "LrJjp4Tnb4OP"
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = random_split(data_obj, [int(0.7 * len(data_obj)), int(0.3 * len(data_obj))], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ehvw1uNy9d0C"
   },
   "outputs": [],
   "source": [
    "batch_data = DataLoader(train_dataset, batch_size = 64)\n",
    "test_batch_data = DataLoader(train_dataset, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "pRobeTQtG_fW"
   },
   "outputs": [],
   "source": [
    "input_size = len(alpha_dict)\n",
    "hidden_size = len(alpha_dict)\n",
    "num_classes = len(alpha_dict)\n",
    "num_epochs = 2\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "VmB27aDyG7GN"
   },
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCivPg3-p-Cl",
    "outputId": "c23ca6ae-55ff-4efb-b9a7-22e702683d92"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 27804/27804 [00:45<00:00, 610.80it/s]\n",
      " 50%|██████████████████████████████████████████▌                                          | 1/2 [00:45<00:45, 45.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch [1/2], Step[27804/27804], Loss: 3.5491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 27804/27804 [00:39<00:00, 699.74it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:25<00:00, 42.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch [2/2], Step[27804/27804], Loss: 3.5547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(batch_data)\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for i, (text, labels) in enumerate(tqdm(batch_data, position = 0, leave = True)):\n",
    "        text = text.type(torch.FloatTensor).to(device)\n",
    "        labels = labels.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(text)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print ('\\n', f'Epoch [{epoch+1}/{num_epochs}], Step[{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "dYTZMx0xcvj_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 27804/27804 [00:21<00:00, 1290.50it/s]\n",
      "/home/biocros/marn/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/biocros/marn/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.545538928067662, 0.6653718889368436, nan)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BblKxNRu_MU"
   },
   "source": [
    "***Probability Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "A0xfpb6Y1r85"
   },
   "outputs": [],
   "source": [
    "P = PModel(alpha_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "wdxUlbrxq1CX"
   },
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.30, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "oWoa4iSC7Lyc"
   },
   "outputs": [],
   "source": [
    "P.model_build(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YYeJWx12AXtH",
    "outputId": "0c0997e6-67e9-44a1-d5f7-b1385803adee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2591196799408347"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.model_test(xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_9HOhPbaCMv"
   },
   "source": [
    "***RNN-Transducer Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "8sD3kslxm8SY"
   },
   "outputs": [],
   "source": [
    "input_size = len(alpha_dict)\n",
    "encoder_dim = len(alpha_dict)\n",
    "joiner_dim = len(alpha_dict)\n",
    "predictor_dim = len(alpha_dict)\n",
    "\n",
    "num_classes = len(alpha_dict)\n",
    "num_epochs = 1\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
